{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "```kewords: tensor, 텐서 연산, 미분, Gradient Descent, ...```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from tensorflow.keras.datasets import mnist\r\n",
    "\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 신경망과의 첫 만남"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "MNIST\r\n",
    "\r\n",
    "- 흑백 손글씨 숫자 이미지\r\n",
    "- 1980s, 미국 국립표준기술연구소 (National Institute of Standards and Technology, NIST)\r\n",
    "- train: 6만 / test: 1만\r\n",
    "- 28, 28, 1\r\n",
    "- 10개의 범주 (0 ~ 9)\r\n",
    "- 딥러닝계의 \"hello world\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(train_images.shape, len(train_labels))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28) 60000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_labels"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(test_images.shape, len(test_labels))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 28, 28) 10000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "test_labels"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 데이터 전처리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\r\n",
    "train_images = train_images.astype(\"float32\") / 255\r\n",
    "\r\n",
    "test_images = test_images.reshape((10000, 28 * 28))\r\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_labels = to_categorical(train_labels)\r\n",
    "test_labels = to_categorical(test_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "network = Sequential()\r\n",
    "network.add(Dense(512, input_shape=(28 * 28,), activation=\"relu\"))\r\n",
    "network.add(Dense(10, activation=\"softmax\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "network.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "신경망의 핵십 구성 요소 = 일종의 데이터 처리 필터, 층 (layer)<br />\r\n",
    "= 주어진 문제에 더 의미 있는 표현(representation)을 입력된 데이터로부터 추출.\r\n",
    "\r\n",
    "대부분의 딥러닝은 간단한 층을 연결하여 구성되며, 점진적으로 데이터를 정제하는 형태를 띤다.<br />\r\n",
    "따라서 데이터 정제 필터(층)가 연속되어 있는 데이터 프로세싱을 위한 여과기 같음."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 완전 연결(fully connected)된 신경망 층인 Dense\r\n",
    "- Softmax, exponential과 구성비를 이용한 class에 대한 확률값을 반환"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "compile을 위해 필요한 3가지\r\n",
    "- 손실 함수 loss function: 옳은 방햐응로 학습될 수 있도록 도움.\r\n",
    "- optimizer: 가중치를 update하는 algorithm\r\n",
    "- 훈련 과정을 monitoring할 지표"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "network.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "network.fit(train_images, train_labels, batch_size=128, epochs=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018F9C4C9AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018F9C4C9AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2566 - accuracy: 0.9255\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1029 - accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0674 - accuracy: 0.9798\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0492 - accuracy: 0.9853\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0374 - accuracy: 0.9888\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18f9c4ed708>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\r\n",
    "print(\"test accuracy:\", test_acc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018F9C6E13A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018F9C6E13A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9790\n",
      "test accuracy: 0.9789999723434448\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "과대적합 (Overfitting): 모델이 훈련 데이터보다 새로운 데이터에서 성능이 낮아지는 경향"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 신경망을 위한 데이터 표현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tensor\r\n",
    "\r\n",
    "- 데이터를 위한 container, 임의의 차원 개수를 가지는 행렬의 일반화된 형태\r\n",
    "- 차원 = 축, 랭크"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Scalar (0D tensor)\r\n",
    "\r\n",
    "- 스칼라 (tensor), 0차원 텐서, 0D 텐서\r\n",
    "- 숫자 하나만 담고 있는 tensor\r\n",
    "- in numpy, float32, float64 type"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "x = np.array(12)\r\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "x.ndim"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Vector (1D tensor)\r\n",
    "\r\n",
    "- 벡터, 1D 텐서"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "x = np.array([12, 3, 6, 14, 7])\r\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([12,  3,  6, 14,  7])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "x.ndim"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Matrix (2D tensor)\r\n",
    "\r\n",
    "- 행렬, 2D 텐서\r\n",
    "- 2개의 축 (행-row과 열-column)\r\n",
    "- 숫자가 채워진 사각 격자"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "x = np.array([[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]])\r\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 5, 78,  2, 34,  0],\n",
       "       [ 6, 79,  3, 35,  1],\n",
       "       [ 7, 80,  4, 36,  2]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "x.ndim"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. 3D tensor ~\r\n",
    "\r\n",
    "- 3D 텐서는 숫자가 채워진 직육면체 형태로 해석할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "x = np.array(\r\n",
    "    [[[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]],\r\n",
    "     [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]],\r\n",
    "     [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]]]\r\n",
    ")\r\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "x.ndim"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 핵심\r\n",
    "\r\n",
    "- 축, rank    : np.array(...).ndim\r\n",
    "- 크기, shape : tensor의 축에 따라 얼마나 많은 차원이 있는지, np.array(...).shape\r\n",
    "- 데이터 타입  : np.array(...).dtype\r\n",
    "  - numpy와 여러 library들이 메모리 사전 할당 문제로 가변 길이의 문자열을 지원하지 않는다.\r\n",
    "  - C언어도 아닌데 동적으로 짜면 느려진단 소리로 보인다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_images.ndim)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_images.dtype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "8bit 정수형"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "digit = train_images[4]\r\n",
    "plt.imshow(digit)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### in numpy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "slicing: 배열에 있는 특정 원소를 선택하는 것"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_slice1 = train_images[10:100]\r\n",
    "my_slice2 = train_images[10:100, :, :]\r\n",
    "my_slice3 = train_images[10:100, 0:28, 0:28]\r\n",
    "print(my_slice1.shape, my_slice2.shape, my_slice3.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_slice4 = train_images[4, 14:, 14:]\r\n",
    "plt.imshow(my_slice4)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_slice5 = train_images[4, 7:-7, 7:-7]\r\n",
    "plt.imshow(my_slice5)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 배치 데이터\r\n",
    "\r\n",
    "```batch = train_images[size * n : size * (n + 1)```\r\n",
    "\r\n",
    "- 한번에 처리하는 데이터의 양\r\n",
    "- 딥러닝에서 첫번째 축은 샘플 축이지만, 배치 데이터를 이용할 땐 배치 축(차원)이라 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 실제\r\n",
    "\r\n",
    "| name | tensor | axis | description |\r\n",
    "|:----:|:------:|------|-------------|\r\n",
    "| vector | 2d tensor | (samples, features) | 가장 기본적인 dataset, cell 표현<br />- 나이에 따른 소득세 데이터셋 |\r\n",
    "| 시계열 or Sequence(주기성) 데이터 | 3d tensor | (samples, timesteps, features) | 시간이나 연속된 순서가 중요할 때 쓰임<br />- 주식 가격 데이터셋, 트윗 데이터셋 |\r\n",
    "| image | 4d tensor | (samples, height, width, channels)<br />(samples, channels, height, width) | 픽셀을 표현하는 색 차원의 추가<br />- tensorflow: channel-last<br />- Theano: channel-first<br />- Keras는 둘 다 가능 |\r\n",
    "| video | 5d tensor | (sampels, frames, height, width, channels)<br />(samples, frames, channels, height, width) | 프레임의 연속이고, 컬러니까 |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 신경망의 톱니바퀴: 텐서 연산"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "모든 신경망 연산은 작은 텐서의 연산으로 나타낼 수 있다. 마치 파이나 곱셈을 덧셈으로 표현할 수 있듯이 말이다.\r\n",
    "\r\n",
    "- ```keras.layers.Dense(512, activation=\"relu\")```\r\n",
    "- output = relu(dot(W, input) + b)\r\n",
    "   - ```dot(W, input)``` | ```Z + b``` | ```relu(Z')```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 원소별 계산\r\n",
    "\r\n",
    "- element-wise operation\r\n",
    "- numpy 연산은 (System's) BLAS에 위임해 병렬 연산을 처리함.\r\n",
    "- BLAS: Basic Linear Algebra Subprogram (Fortan or C)\r\n",
    "- 같은 위치에 있는 얘들끼리 덧셈 및 곱셈을 하는 것."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_add(x, y):\r\n",
    "    assert len(x.shape) == 2\r\n",
    "    assert x.shape == y.shape\r\n",
    "\r\n",
    "    x = x.copy()\r\n",
    "    for i in range(x.shape[0]):\r\n",
    "        for j in range(x.shape[1]):\r\n",
    "            x[i, j] += y[i, j]\r\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_relu(x):\r\n",
    "    assert len(x.shape) == 2\r\n",
    "\r\n",
    "    x = x.copy()\r\n",
    "    for i in range(x.shape[0]):\r\n",
    "        x[i, j] = max(x[i, j], 0)\r\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### broadcasting\r\n",
    "\r\n",
    "- 큰 tensor의 input에 맞도록 작은 tensor에 (broadcasting 축이라고 부르는) 축이 추가됨.\r\n",
    "- 작은 tensor가 새 축을 따라서 큰 tensor의 크기에 맞도록 반복됨."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<pre>\r\n",
    "assert x.shape == (32, 10)\r\n",
    "assert y.shape == (10,)\r\n",
    "\r\n",
    "y.reshape((0, -1))\r\n",
    "-> y.shape == (1, 10)\r\n",
    "y[i, :] = y for i in range(x.shape[0])\r\n",
    "\r\n",
    "return x + y\r\n",
    "</pre>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_add_matrix_and_vector(x, y):\r\n",
    "    assert len(x.shape) == 2\r\n",
    "    assert len(y.shape) == 1\r\n",
    "    assert x.shape[1] == y.shape[0]\r\n",
    "\r\n",
    "    x = x.copy()\r\n",
    "    for i in range(x.shape[0]):\r\n",
    "        for j in range(x.shape[1]):\r\n",
    "            x[i, j] += y[i]\r\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텐서 곱셈\r\n",
    "\r\n",
    "- tensor product\r\n",
    "- 원소별 연산과 반대로 입력 텐서의 원소들을 결합시킴.\r\n",
    "- (a, b, c, d) * (d, ) = (a, b, c)\r\n",
    "- (a, b, c, d) * (d, e) = (a, b, c, e)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_vecetor_dot(x, y):\r\n",
    "    assert len(x.shape) == 1\r\n",
    "    assert len(y.shape) == 1\r\n",
    "    assert x.shape[0] == y.shape[0]\r\n",
    "\r\n",
    "    z = 0.\r\n",
    "    for i in range(x.shape[0]):\r\n",
    "        z += x[i] * y[i]\r\n",
    "    return z"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_matrix_vector_dot(x, y):\r\n",
    "    assert len(x.shape) == 2\r\n",
    "    assert len(y.shape) == 1\r\n",
    "    assert x.shape[1] == y.shape[0]\r\n",
    "\r\n",
    "    z = np.zeros(x.shape[0])\r\n",
    "    for i in range(x.shape[0]):\r\n",
    "        for j in range(x.shape[1]):\r\n",
    "            z[i] += x[i, j] * y[j]\r\n",
    "    return z"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_matrix_vector_dot(x, y):\r\n",
    "    z = np.zeros(x.shape[0])\r\n",
    "    for i in range(x.shape[0]):\r\n",
    "        z[i] = naive_vector_add(x[i, :], y)\r\n",
    "    return z"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_matrix_dot(x, y):\r\n",
    "    assert len(x.shape) == 2\r\n",
    "    assert len(y.shape) == 2\r\n",
    "    assert x.shape[1] == y.shape[0]\r\n",
    "\r\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\r\n",
    "    for i in range(x.shape[0]):\r\n",
    "        for j in range(y.shape[1]):\r\n",
    "            row_x = x[i, :]\r\n",
    "            column_y = y[:, j]\r\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\r\n",
    "    return z"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텐서 크기 변환\r\n",
    "\r\n",
    "- 연산 전 전처리 용도랄까.\r\n",
    "- 대체로 **전치(Transposition)**를 가장 많이 이용"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텐서 연산의 기하학적 해석\r\n",
    "\r\n",
    "- tensor는 곧 vector로 변환될 수 있으므로, 모든 tensor 연산은 n-차원에 대한 좌표 포인트로 이해될 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 딥러닝의 기하학적 해석\r\n",
    "\r\n",
    "- 삼단논법에 의거하여, 딥러닝은 텐서 연산의 확장이고 / 텐서 연산은 그 깊이(복잡함)가 얼마든 기학학적 변환으로 해석될 수 있다.\r\n",
    "- 따라서, 딥러닝은 복잡한 텐서 연산의 집합체이므로, 기학학적 변환으로 해석될 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 신경망의 엔진: 그래디언트 기반 최적화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*W*, *b*: 가중치(weight)이자 훈련되는 파라미터(trainable parameter), 각각 커널(kernel)과 편향(bias)\r\n",
    "\r\n",
    "- 초기 가중치 행렬에는 난수로 채워져 있다. (random initialization)\r\n",
    "\r\n",
    "피드백 신호에 기초하여 가중치가 점진적으로 조정된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "gradient update는 신경망에 사용된 모든 연산이 미분 가능(differentiable)하다는 (장)점을 이용한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 변화율\r\n",
    "\r\n",
    "- 연속성이라는 건 f(x) = y가 연결된 함수라서 x의 변화에 따라 y가 같이 변하는 걸 말한다.\r\n",
    "- 변화율이라는 건 지점 p에 대해서 x에 대한 y의 변화 정도를 말한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 그래디언트\r\n",
    "\r\n",
    "- gradient: 텐서 연산의 변화율, 수학적으로 말하면 기울기 또는 기울기의 행렬\r\n",
    "- *W1 = W0 - step \\* gradient(f)(W0)*\r\n",
    "- step: W0에서 너무 크게 벗어나지 않기 위한 scaling ratio"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 확률적 경사 하강법, SGD\r\n",
    "\r\n",
    "- 통계에서 최솟값을 찾는 방식은 수학의 극솟값을 찾는 방식과 동일하다.\r\n",
    "- 그러나 미지수가 10개 넘어가는 다중 모델에서는 n차 식에 대한 연립방정식이 되기에 이와 같은 것이 불가능하다.\r\n",
    "- 그래서 기울기가 0이 되는 지점을 향해 점진적으로 나아가도록 학습시킨다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "training loop (mini-batch stochastic gradient descent)\r\n",
    "\r\n",
    "> 확률적 = 무작위의 과학적 표현\r\n",
    "\r\n",
    "1. 훈련 샘플 x와 이에 상응하는 타깃 y의 배치를 추출합니다.\r\n",
    "2. x를 사용하여 네트워크를 실행하고(정방향 패스(forward pass) 단계), 예측 y_pred를 구합니다.\r\n",
    "3. y_pred와 y의 차이를 측정하여 이 배치에 대한 네트워크의 손실을 계산합니다.\r\n",
    "4. 네트워크의 파라미터에 대한 손실 함수의 그래디언트를 계산합니다(역방향 패스(barckward pass)).\r\n",
    "5. 그래디언트의 반대 방향으로 파라미터를 조금 이동시킵니다. 예를 들어, *W -= step \\* gradient*처럼 하면 배치에 대한 손실이 조금 감소할 것입니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 반복마다 하나의 샘플과 하나의 타깃을 이용 -> (true) SGD\r\n",
    "- 가용한 모든 데이터를 사용하여 반복 -> batch SGD\r\n",
    "- 이 둘의 절충안이 mini-batch SGD\r\n",
    "\r\n",
    "다른 optimizer로는 Momentum, Adagrad, RMSProp, Adam etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "저차원 표현으로 얻은 직관이 실전과 항상 맞지 않는다는 것을 유념해야 한다.\r\n",
    "> 대표적으로 신경망 알고리즘이 지역 최솟값에 쉽게 갇힐 것으로 생각했지만, 고차원 공간에서는 대부분 안장점(saddle point)으로 나타나고 지역 최솟값은 매우 드뭅니다. 예를 들어 1,000개의 파라미터가 있는 공간에서 변화율이 0인 지점의 모든 파라미터가 최솟값일 가능성은 2<sup>-1000</sup>입니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "past_velocity = 0.\r\n",
    "momentum = 0.1 # momentum ratio는 0.9를 보편적으로 사용\r\n",
    "while loss > 0.01:\r\n",
    "    w, loss, gradient = get_current_parameters()\r\n",
    "    velocity = momentum * past_velocity - learning_rate * gradient\r\n",
    "    # w = w + velocity # 기본\r\n",
    "    w = w + momentum * velocity - learning_rate * gradient # Nesterov Momentum, momentum 두 번 반복\r\n",
    "    past_velocity = velocity\r\n",
    "    update_parameter(w)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 역전파 알고리즘\r\n",
    "\r\n",
    "- back propagation, 후진 모드 자동 미분 (reverse_mode automatic differentiation)\r\n",
    "- 최종 손실 값에서부터 각 파라미터가 기여한 정도를 계산하기 위해 연쇄 법칙을 적용하여 최상위 층에서 하위 층까지 거꾸로 진행"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 첫번째 예제 다시 살펴보기\r\n",
    "\r\n",
    "- epoch: 전체 훈련 데이터에 수행되는 각 반복\r\n",
    "- > 훈련 샘플이 6만 개, 배치 사이즈를 128로 했다면, 60,000 = 468 * 128 + 96 (마지막 배치)로 배치 갯수는 469개이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. 요약\r\n",
    "\r\n",
    "- 학습은 훈련 데이터 샘플과 그에 상응하는 타깃의 손실 함수를 최소화하는 파라미터의 조합을 찾는 것.\r\n",
    "- 전체 학습 과정은 미분 가능한 텐서 연산으로 연결되어 있기에 가능. 현재 파라미터와 배치 데이터를 그래디언트 값에 매핑해 주는 그래디언트 함수를 구성하기 위해 미분의 연쇄 법칙을 사용"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}